\documentclass[a4paper,12pt]{article}

\usepackage{cmap}		
%\usepackage[utf8]{inputenc}			
\usepackage[english,brazil]{babel}
\usepackage{framed}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{wrapfig}
\usepackage{lipsum}
\usepackage{listings}
\usepackage{color}
\usepackage{indentfirst}
\usepackage{times}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{pgfgantt}

\newif\ifblackandwhite
\blackandwhitetrue

\usepackage[hmargin=2cm,vmargin=2.5cm]{geometry}
\usepackage{etoolbox}
\usepackage{longtable}%

\usepackage{pdflscape}
%\usepackage[svgnames]{xcolor}
\usepackage{colortbl}%
   \newcommand{\myrowcolour}{\rowcolor[gray]{0.925}}
\usepackage{booktabs}

\definecolor{mygray}{rgb}{0.4,0.4,0.4}
\definecolor{mygreen}{rgb}{0,0.8,0.6}
\definecolor{myorange}{rgb}{1.0,0.4,0}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\newtheorem{definition}{Definição}
\newtheorem{theorem}{Teorema}

\begin{document}

\input{sections/title.tex}

\newpage
%\tableofcontents

\newpage
\section*{Resumo}
\label{sc:resumo}
\noindent
Algoritmos de aprendizado não supervisionado de métricas utilizam grafos como aproximações discretas para as variedades que representam a estrutura geométrica subjacente dos conjuntos de dados multivariados. Atualmente, a metodologia adotada pelos algoritmos tradicionais de aprendizado de variedades na ponderação das arestas desses grafos ainda é bastante rudimentar, uma vez que adota-se a distância Euclidiana como medida de similaridade. Porém, a distância Euclidiana é extrínseca à variedade em questão e não leva em consideração a noção de curvatura. Por essa razão, esse projeto de pesquisa visa propor métodos matematicamente originais, mais precisos e adequados para a caracterização da similaridade entre amostras vizinhas do grafo KNN. A ideia consiste em utilizar conceitos da geometria diferencial, como a curvatura, que é uma medida intrínseca, para ponderar arestas de caminhos mínimos em tais grafos, uma vez que eles representam aproximações para as verdadeiras distâncias geodésicas entre diferentes pontos pertencentes à variedade. Basicamente, a estratégia consiste em medir as variações dos espaços tangentes conforme nos movemos através de um caminho mínimo no grafo KNN. Com isso, espera-se melhorar o desempenho de diversos algoritmos utilizados na extração de características em problemas de classificação de padrões, como o PCA (\emph{Principal Component Analysis}), ISOMAP (\emph{Isometric Feature Mapping}) e LLE (\emph{Locally Linear Embedding}). Resultados preliminares com o algoritmo ISOMAP mostram um ganho significativo na acurácia da classificação de diversos conjuntos de dados reais em comparação com as versões tradicionais e outros métodos estado da arte, como t-SNE e UMAP. Além disso, espera-se incorporar os grafos baseados em curvatura (\emph{K-graphs}) em modelos para classificação de padrões e agrupamento de dados baseados em grafos.  

\section*{Abstract}
\noindent
Unsupervised metric learning algorithms use graphs as discrete approximations for the manifolds that represent the underlying geometric structure of multivariate data. At the moment, the usual methodology employed by these algorithms for building the KNN graph edges is quite elementary, since the Euclidean distante is the similarity measure. However, the Euclidean distance is extrinsic to the manifold and it does not take into account the intrinsic notion of curvature. For this reason, this research project aims to propose original and more suitable mathematical methods to characterize the similarity between neighboring samples in the KNN graph. The idea consists in the application of differential geometry concepts, such as the curvature, an intrinsic measure, to weight the edges of shortest paths in the graphs, since they represent discrete approximations to the true underlying geodesic distances between different points belonging to the manifold. Basically, our strategy is to measure the variation of the tangent space as we move along a shortest path in the KNN graph. With the proposed method, we expect to improve the performance of several feature extraction algorithms in pattern classification, such as PCA (\emph{Principal Component Analysis}), ISOMAP (\emph{Isometric Feature Mapping}) and LLE (\emph{Locally Linear Embedding}). Preliminary results with the ISOMAP algorithm show a significant gain in the classification accuracy of several real world datasets in comparison to their regular versions and other state-of-the-art methods, such as t-SNE and UMAP. Moreover, we intend to incorporate the proposed curvature based graphs (\emph{K-graphs}) in graph-based classification and clustering models.

\newpage

\section{Enunciado do problema}
\noindent
O aprendizado não supervisionado de métricas tem como objetivo final a estimação de funções de distância adaptativas aos conjuntos de dados previamente à etapa de classificação. Por funções de distância adaptativas entenda-se uma medida de similaridade que leve em consideração a estrutura geométrica intrínseca de cada conjunto de dados. A principal classe de métodos computacionais para esse fim são os algoritmos de aprendizado de variedades, ou do inglês, \emph{manifold learning} \cite{Seung,Brand,Cayton,Huo2007,Advances,Diffusion1}. Alguns dos representantes mais conhecidos de tais métodos são os algoritmos \emph{Isometric Feature Mapping} (ISOMAP) \cite{Isomap,Isomap_converg}, \emph{Locally Linear Embedding} (LLE) \cite{LLE,LLE2,LLE3,LLE4, LLE5} e \emph{Laplacian Eigenmaps} \cite{LapEig,SurveyLE}. Uma etapa comum a todos eles consiste na construção de um grafo KNN, em que deve ser criada uma aresta entre cada amostra da base e seus $K$ vizinhos mais próximos. Em todas as versões tradicionais desses algoritmos, pondera-se as arestas de tal grafo com a distância Euclidiana entre os vetores no espaço de características, sob a justificativa de que uma vizinhança local do grafo KNN pode ser bem representada por um patch linear no espaço de atributos. Porém, é conhecido que se o valor de $K$ não é tão pequeno ou se a curvatura local na variedade é elevada, essa hipótese de linearidade torna-se bastante frágil.

Diante do exposto, a pergunta que surge é: como ponderar as arestas dos grafos KNN em algoritmos para aprendizado não supervisionado de métricas de maneira mais precisa, substituindo a distância Euclidiana (que é extrínseca) por uma medida intrínseca à variedade que representa a estrutura geométrica oculta do conjunto de dados?

A proposta desse projeto de pesquisa consiste em adotar como ferramenta matemática a geometria diferencial, mais precisamente a noção de curvatura de uma curva paramétrica, para aproximar as distâncias geodésicas na variedade por caminhos mínimos no grafo KNN. A relevância, motivação, objetivos e metodologia proposta são discutaidas em detalhe nas seções subsequentes deste projeto de pesquisa.

\section{Relevância científica e motivação}
\label{sc:motivacao}
\noindent
Conjuntos de dados multivariados são, em geral, compostos de um número finito de amostras que podem conter um elevado número de atributos. Se os efeitos de um aumento no número de amostras são benéficos para processos de aprendizado, os efeitos de um aumento arbitrário no número de atributos podem ser bastante negativos, em especial em tarefas de classificação de padrões \cite{Debie}. Um dos maiores problemas com a análise de dados de alta dimensão é a maldição da dimensionalidade, que refere-se ao fato de que para a correta estimação de uma função de várias variáveis, o tamanho da amostra $n$ precisa crescer com o número de variáveis, $m$ \cite{Bellman:61}. Em classificadores paramétricos, pode-se mostrar que $n$ pode ser uma função quadrática de $n$ e em classificadores não paramétricos, a situação é ainda pior, uma vez que $n$ pode ser uma função exponencial de $m$ \cite{Fukunaga,Scott92,Hwang94}. Assim, como nem sempre é possível obter um número de amostras elevado, uma maneira natural de superar esse problema, e indiretamente reduzir o valor de $n$, é reduzir a dimensionalidade dos dados, ou seja, $m$.

Outros efeitos colaterais negativos da maldição da dimensionalidade são: o fenômeno do espaço vazio, causado majoritariamente pela esparsidade do espaço de características original, e o fenômeno de Hughes. Pode-se mostrar que no caso de um número finito de amostras $n$, cenário recorrente nas aplicações práticas, existe uma dimensionalidade ótima $m^{*}$, acima da qual a acurácia média da classificação supervisionada decresce \cite{Hughes68}. Portanto, encontrar o número ótimo de atributos é uma etapa fundamental para minimizar erros indesejáveis em sistemas de reconhecimento de padrões.

Diversos métodos para aprendizado não supervisionado de métricas via redução de dimensionalidade existem na literatura, dentre os quais podemos citar a Análise de Componentes Principais, ou PCA (\emph{Principal Component Analysis}), o ISOMAP (\emph{Isometric Feature Mapping}), o LLE (\emph{Locally Linear Embedding}) e a técnica \emph{Laplacian Eigenmaps}. Métodos de redução de dimensionalidade estão profundamente conectados com o aprendizado não supervisionado de métricas, no sentido de que além de tais métodos conseguirem aprender uma representação mais compacta e relevante para os dados observados, eles também podem aprender uma função de distância que geometricamente é mais adequada para representar uma medida de similaridade entre pares de objetos pertencentes a essa coleção de amostras \cite{MetricLearning1,MetricLearning2}.

Uma limitação comum a grande maioria desses métodos é a hipótese de que uma vizinhança local (patches do grafo KNN) formam um espaço linear, ou seja, assumem a métrica Euclidiana como função de distância, o que pode causar diversos problemas. Um deles está relacionado com o fraco poder discriminante da métrica Euclidiana. Pode-se mostrar que, conforme $m$ aumenta, o contraste fornecido pela distância Euclidiana diminui significativamente, fazendo com que a distribuição das normas em um conjunto de pontos tenda a se concentrar, originando o fenômeno da concentração \cite{Verleysen}. Outro problema é que em pontos de alta curvatura, essa hipótese torna-se bastante fraca, no sentido de que a métrica Euclidiana não produz funções de distâncias adequadas. 

Nesse contexto, a proposta deste projeto consiste em propor novas medidas de distâncias para substituir a distância Euclidiana, que é considerada uma métrica extrínseca, uma vez que utiliza pontos de fora da variedade que define a verdadeira estrutura geométrica oculta dos dados observados. Podemos fazer a analogia com a distância entre dois pontos na superfície terrestre: a distância Euclidiana seria definida por uma reta, o que não levaria em consideração a curvatura presente na superfície da terra, que obviamente, não é plana (embora ainda existam defensores dessa ideia absurda!).

\section{Objetivos}
\label{sc:objetivos}

%1. Aprimorar algoritmos para aprendizado de métricas não supervisionado baseados em redução de dimensionalidade
%	a. Substituir a distância Euclidiana pontual por uma métrica mais adequada (intrínseca e baseada em patches)
%	b. Tornar os algoritmos de aprendizado de métricas não supervisionados mais robustos em relação a presença de ruídos e outliers nos dados
%	c. Propor metodologias mais precisas baseadas em conceitos da geometria diferencial, como a curvatura local, para mensurar a dissimilaridade
%	d. Definir os grafos baseados em curvatura (KKNN-graphs), em que as arestas são deformadas quanto maior for a curvatura entre dois pontos
%
%2. Comparar a qualidade dos agrupamentos obtidos após as reduções de dimensionalidade com os métodos originais e propostos
%
%3. Comparar as acurácias da classificação supervisionadas obtidas após as reduções de dimensionalidade com os métodos originais e propostos


Neste projeto de pesquisa, propõe-se uma nova técnica para a ponderação das arestas de grafos criados a partir de conjuntos de dados multivariados utilizando distâncias intrinsecas ao invés da distância Euclidiana (extrínseca). Com esse novo modelo de grafo, espera-se obter uma melhor aproximação discreta para as verdadeiras distâncias geodésicas na variedade subjacente, o que seria benéfico para diversos algoritmos de aprendizado não supervisionado de métricas baseados em redução de dimensionalidade, como o PCA, ISOMAP, LLE, \emph{Laplacian Eigenmaps}, dentre outros.

De maneira geral, pretende-se melhorar principalmente dois aspectos fundamentais dos métodos tradicionais: 1) torná-los menos sensíveis a presença de ruídos e \emph{outliers} nos dados, uma vez que, ao contrário da distância Euclidiana, que é pontual, as medidas baseadas em conceitos da geometria diferencial são contextuais, isto é, são calculadas entre patches; e 2) aumentar tanto a qualidade dos agrupamentos quanto o poder discriminante dos atributos, de modo a conseguir melhoria de desempenho na classificação de conjuntos de dados. 

\subsection{Objetivos gerais}

Desenvolver novas técnicas para o aprendizado não supervisionado de métricas baseado na extensão de algoritmos de redução de dimensionalidade linear e não linear, tais como, PCA, ISOMAP, LLE e \emph{Laplacian Eigenmaps}, validando-as em diversas bases de dados publicas disponíveis na internet, como por exemplo, no repositório \url{www.openml.org}. Em resumo, o objetivo geral desse projeto de pesquisa consiste em testar a seguinte hipótese de pesquisa:

\begin{itemize}
	\item A incorporação de medidas intrínsecas (não Euclidianas) baseadas em conceitos da geometria diferencial nos métodos de aprendizado de métricas não supervisionados é capaz de trazer melhorias significativas em problemas de classificação de padrões e análise de agrupamentos.
\end{itemize}

Para obter respostas a essa questão norteadora, novos métodos serão propostos e análises estatísticas serão utilizadas para compará-los com suas versões Euclidianas. A seção a seguir mostra como as pesquisas devem proceder para que a hipótese de pesquisa seja esclarecida. 

\subsection{Objetivos específicos}

Os objetivos específicos, ajudam a guiar o desenvolvimento e a organização do projeto. Ao longo da pesquisa pretende-se trabalhar com o intuito de atingir os seguintes objetivos específicos:

\begin{itemize}
	\item Propor e desenvolver um novo modelo baseado em grafos, denominado de \emph{K-graphs}, para definir uma aproximação discreta à estrutura geométrica oculta dos dados (variedade) utilizando para isso o conceito de curvatura da geometria diferencial;
	\item Propor e desenvolver o método K-PCA, uma generalização dos algoritmos Parametric PCA \cite{PPCA} e PCA-KL \cite{PCAKL}, também desenvolvidos pelo proponente em trabalhos recentes;
	\item Avaliar e comparar o método K-PCA com métodos similares encontrados na literatura como PCA tradicional \cite{PCA}, Kernel PCA \cite{KPCA}, \textit{Sparse PCA} \cite{SparsePCA2006}, \textit{Robust PCA} \cite{RobustPCA}, ISOMAP \cite{Isomap}, LLE \cite{LLE}, Laplacian Eigenmaps \cite{LapEig}, t-SNE \cite{tSNE} e UMAP \cite{UMAP} em problemas de classificação;
	\item Propor e desenvolver o método K-ISOMAP, uma generalização do método ISOMAP-KL \cite{ISOKL}, também proposto pelo proponente e seu aluno de doutorado Alaor Cervati Neto;
	\item Avaliar e comparar o método K-ISOMAP com métodos similares encontrados na literatura como PCA, Kernel PCA, \textit{Sparse PCA}, \textit{Robust PCA}, ISOMAP, LLE, Laplacian Eigenmaps, t-SNE e UMAP em problemas de classificação;
	\item Propor e desenvolver o método K-LLE, uma generalização do método LLE que incorpora o modelo K-graphs;
	\item Avaliar e comparar o método K-LLE com métodos similares encontrados na literatura como PCA, Kernel PCA, \textit{Sparse PCA}, \textit{Robust PCA}, ISOMAP, LLE, Laplacian Eigenmaps, t-SNE e UMAP em problemas de classificação;		
%	\item Propor e desenvolver o método K-Laplacian, uma generalização do método Laplacian Eigenmaps que incorpora o modelo K-graphs;
%	\item Avaliar e comparar o método K-Laplacian com métodos similares encontrados na literatura como PCA, Kernel PCA, \textit{Sparse PCA}, \textit{Robust PCA}, ISOMAP, LLE, Laplacian Eigenmaps, t-SNE e UMAP em problemas de classificação;
\end{itemize}


%\newpage
\section{Metodologia proposta}
\label{sc:metodologia}
\noindent
Esta seção apresenta o ferramental matemático necessário para a construção dos modelos utilizados na metodologia proposta, bem como uma proposta inicial para o método K-ISOMAP, uma generalização do ISOMAP que utiliza uma métrica intrínseca baseada na curvatura local em cada vértice do grafo KNN. O objetivo consiste em aperfeiçoar o aprendizado não supervisionado de métricas via redução de dimensionalidade.

\subsection{Geometria diferencial de curvas}

Esta seção apresenta uma breve discussão sobre os principais conceitos da geometria diferencial de curvas  \cite{ElementaryDG,FirstCourseDG,Manfredo}. Seja $\vec{\alpha}(t) = (x(t), y(t), z(t))$, com $\vec{\alpha}: [a, b] \rightarrow R^3$ uma curva paramétrica. Para caracterizar uma curva paramétrica sem a utilização de variáveis extrínsecas, é necessário a definição de conceitos da geometria diferencial de curvas. De modo formal, isso pode ser feito através da construção do triedro de Frenet, uma base ortogonal adaptativa composta pelos vetores tangente, normal e binormal, que se ajusta a cada ponto da curva ao nos movermos sobre ela. 

\begin{definition}
O vetor tangente de uma curva paramétrica $\vec{\alpha}(t)$ é definido por:
\begin{equation}
	\vec{\alpha}'(t) = \left( x'(t), y'(t), z'(t) \right)
\end{equation} onde $f'(t)$ a derivada primeira de $f(t)$.
\end{definition}

\subsubsection{Comprimento de arco e curvatura}

Uma importante medida relacionada a curvas paramétricas é o comprimento de arco. A Figura \ref{fig:arc_length} ilustra como o comprimento de arco entre $t = r$ e $t = s$ pode ser aproximado pela norma do vetor $\vec{s}$, que é a diferença entre $\vec{\alpha}(r)$ e $\vec{\alpha}(s)$.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[scale=0.6]{./img/arc_length.png}}
\caption{O cálculo do comprimento de arco pode ser aproximado pela norma do vetor diferença (hipotenusa de um triângulo retângulo).}
\label{fig:arc_length}
\end{center}
\end{figure}

Se o número de subdivisões do intervalo $[r, s]$ for aumentado, mais precisa será a aproximação, dada por:

\begin{equation}
	s(t) \approx \sum_{t=r}^{s} \sqrt{ \left( x(t+\Delta t) - x(t) \right)^2 + \left( y(t+\Delta t) - y(t) \right)^2 + \left( z(t+\Delta t) - z(t) \right)^2 }
\end{equation}

Note que é possível reescrever $s(t)$ como:

\begin{equation}
	s(t) \approx \sum_{t=r}^{s} \sqrt{ \left( \frac{x(t+\Delta t) - x(t)}{\Delta t} \right)^2 + \left( \frac{y(t+\Delta t) - y(t)}{\Delta t} \right)^2 + \left( \frac{z(t+\Delta t) - z(t)}{\Delta t} \right)^2 } \Delta t
\end{equation}

Pela definição da derivada de uma função, temos:

\begin{equation}
	f'(t) = \lim\limits_{\Delta t \to 0} \frac{f(t+\Delta t) - f(t)}{\Delta t}
\end{equation}

Então, no caso limite, quando $\Delta t \to 0$, a aproximação se torna exata:

\begin{align}
	s(t) & = \int_{a}^{b} \sqrt{ x'(t)^2 + y'(t)^2 + z'(t)^2 } dt \\ \nonumber & = \int_{a}^{b} \sqrt{ \vec{\alpha}'(t)^T \vec{\alpha}'(t)  } dt = \int_{a}^{b} \lVert \vec{\alpha}'(t) \rVert dt
\end{align} onde o integrando $\lVert \vec{\alpha}'(t) \rVert$ é a norma do vetor tangente no ponto $t$.

Em muitas aplicações, é mais natural parametrizar a curva pelo comprimento de arco. A ideia consiste em ter um mapeamento 1-1 entre o tempo (t) e o espaço (s), de modo que ao invés de termos $t \in [a, b]$, temos $s \in [0, L]$, onde $L = \int_{a}^{b} \lVert \vec{\alpha}'(t) \rVert dt$. Em termos físicos, a parametrização por comprimento de arco é equivalente a se mover ao longo da curva com velocidade constante e igual a 1, ou seja:

\begin{equation}
	\vec{\alpha}'(s) = \frac{\vec{\alpha}'(t)}{\lVert \vec{\alpha}'(t) \rVert} = \vec{T}(t)
\end{equation}

Basicamente, em termos matemáticos, a ideia com a parametrização por comprimento de arco é ter os campos normal e tangente normalizados, ou seja, seus vetores possuem norma unitária.

\begin{theorem}
O vetor normal, que é a derivada segunda de $\vec{\alpha}(t)$, é ortogonal ao vetor tangente.
\end{theorem}

Note que $\lVert \vec{\alpha}'(s) \rVert^2 = 1$, o que implica em:

\begin{equation}
	x'(s)^2 + y'(s)^2 + z'(s)^2 = 1
\end{equation}

Derivando em relação a $s$ em ambos os lados e dividindo por 2, temos:

\begin{equation}
	x'(s)x''(s) + y'(s)y''(s) + z'(s)z''(s) = 0
\end{equation} o que finalmente leva a:

\begin{equation}
	\vec{\alpha}'(s)^T \vec{\alpha}''(s) = 0
\end{equation}

Sabendo que $\vec{\alpha}'(s) = \vec{T}(t)$, temos $\vec{\alpha}''(s) = \vec{T}'(t)$, o que mostra que o campo normal é de fato a variação do campo tangente. Para normalizar o vetor normal, define-se:

\begin{equation}
	\vec{N}(t) = \frac{\vec{T}'(t)}{\lVert \vec{T}'(t) \rVert} = \frac{\vec{\alpha}''(s)}{\lVert \vec{\alpha}''(s) \rVert}
\end{equation}

\begin{definition}
A curvatura de $\vec{\alpha}(s)$ no ponto $s \in [0, L]$ é a taxa de variação do vetor tangente unitário neste ponto, dada por:
\begin{equation}
	K(s) = \lVert \vec{\alpha}''(s) \rVert = \lVert \vec{T}'(t) \rVert
\end{equation}
\end{definition}

Para que seja possível expressar a curvatura em termos de quantidades conhecidas, primeiramente note que:

\begin{equation}
	\vec{\alpha}'(s) = \frac{\vec{\alpha}'(t)}{\lVert \vec{\alpha}'(t) \rVert}
\end{equation} o que leva a:

\begin{equation}
	\vec{\alpha}'(t) = \lVert \vec{\alpha}'(t) \rVert \vec{\alpha}'(s)
	\label{eq:star}
\end{equation}

Pela definição do comprimento de arco, sabe-se que:

\begin{equation}
	s = \int_{a}^{b} \lVert \vec{\alpha}'(t) \rVert dt
\end{equation} de modo que pelo Teorema Fundamental do Cálculo temos:

\begin{equation}
	\frac{ds}{dt} = \lVert \vec{\alpha}'(t) \rVert
	\label{eq:twostar}
\end{equation}

Inserindo a equação \eqref{eq:twostar} em \eqref{eq:star}:

\begin{equation}
	\vec{\alpha}'(t) = \frac{ds}{dt} \vec{\alpha}'(s)
\end{equation}

Derivando ambos os lados da equação anterior com relação a $t$, pela regra do produto temos:

\begin{align}
	\vec{\alpha}''(t) = \frac{d^2 s}{dt^2} \vec{\alpha}'(s) + \frac{ds}{dt} \frac{d}{ds} \vec{\alpha}'(s) \frac{ds}{dt} = \frac{d^2 s}{dt^2} \vec{\alpha}'(s) + \left( \frac{ds}{dt} \right)^2 \vec{\alpha}''(s)
\end{align}

Neste ponto, pode-se computar o produto vetorial com relação a $\vec{\alpha}'(t)$ em ambos os lados:

\begin{align}
	\vec{\alpha}'(t) \times \vec{\alpha}''(t) = \vec{\alpha}'(t) \times \left[ \frac{d^2 s}{dt^2} \vec{\alpha}'(s) + \left( \frac{ds}{dt} \right)^2 \vec{\alpha}''(s) \right]
\end{align}

Como os vetores $\vec{\alpha}'(s)$ e $\vec{\alpha}'(t)$ são paralelos, ou seja, eles apontam para a mesma direção, o produto vetorial entre eles é nulo, o que resulta em:

\begin{align}
	\vec{\alpha}'(t) \times \vec{\alpha}''(t) = \left( \frac{ds}{dt} \right)^2 \left[ \vec{\alpha}'(t) \times \vec{\alpha}''(s) \right]
\end{align}

Da equação \eqref{eq:twostar}, podemos escrever:

\begin{equation}
	\vec{\alpha}'(t) \times \vec{\alpha}''(t) = \lVert \vec{\alpha}'(t) \rVert^2 \left[ \vec{\alpha}'(t) \times \vec{\alpha}''(s) \right]
\end{equation}

Calculando a norma em ambos os lados, tem-se:

\begin{equation}
	\lVert \vec{\alpha}'(t) \times \vec{\alpha}''(t) \rVert = \lVert \vec{\alpha}'(t) \rVert^2 \lVert \vec{\alpha}'(t) \rVert \lVert \vec{\alpha}''(s) \rVert sin(\theta)
\end{equation} onde $\theta$ é o ângulo entre $\vec{\alpha}'(t)$ e $\vec{\alpha}''(s)$. Como se sabe, os vetores tangente e normal são ortogonais, o que finalmente nos leva a expressão para o cálculo da curvatura no ponto, denotada por $K(t)$:

\begin{equation}
	K(s) = \lVert \vec{\alpha}''(s) \rVert = \frac{\lVert \vec{\alpha}'(t) \times \vec{\alpha}''(t) \rVert}{\lVert \vec{\alpha}'(t) \rVert^3}
\end{equation}

A seguir, é discutido como a curvatura $K(s)$ desempenha um papel importante no estudo de variações no campo tangente, o que na proposta desta pesquisa é representado pelo espaço tangente em um dado ponto em uma variedade M.

\subsubsection{As equações de Frenet-Serret}

Seja $\vec{B}(t) = \vec{T}(t) \times \vec{N}(t)$ o vetor binormal unitário. Então, o conjunto de vetores $\{ \vec{T}(t), \vec{N}(t), \vec{B}(t) \}$ define uma base ortonormal em cada ponto $t \in [a, b]$. Este conjunto é conhecido como o Triedro de Frenet, um referencial móvel e adaptativo que se ajusta a curva paramétrica $\vec{\alpha}(t)$. A ideia principal das equações de Frenet-Serret é expressar a taxa de variação do Triedro de Frenet em termos de si próprio \cite{Serret,Frenet}.

\subsubsection*{Variação do campo tangente}

A pergunta que deseja-se responder aqui é: o que faz o vetor tangente mudar de um ponto $t$ para um outro ponto vizinho $t'$? Lembre-se que:

\begin{equation}
	\vec{T}'(t) = \lVert \vec{T}'(t) \rVert \vec{N}(t) = K(t) \vec{N}(t)
\end{equation} mostrando que a variação no campo tangente é expressada somente em termos do campo normal, através da curvatura $K(t)$. Assim, para entender como os vetores tangentes se movem ao longo da curva, é preciso analisar a curvatura em cada ponto.

\subsubsection*{Variação do campo binormal}

Pode-se expandir $\vec{B}'(t)$ nos componentes do Triedro de Frenet como:

\begin{equation}
	\vec{B}'(t) = c_1 \vec{T}(t) + c_2 \vec{N}(t) + c_3 \vec{B}(t)
\end{equation}

Como a norma do vetor binormal é unitária, temos $\vec{B}(t)^T \vec{B}(t) = 1$. Derivando em relação a $t$, podemos escrever:

\begin{equation}
	\vec{B}'(t)^T \vec{B}(t) + \vec{B}(t)^T \vec{B}'(t) = 0
\end{equation} o que leva a:

\begin{equation}
	2 \vec{B}'(t)^T \vec{B}(t) = 0
\end{equation} mostrando que $\vec{B}'(t)$ é ortogonal a $\vec{B}(t)$ e portanto $c_3 = 0$. Sabendo que $\vec{B}(t)^T \vec{T}(t) = 0$, pela derivação de ambos os lados em relação a $t$, temos:

\begin{equation}
	\vec{B}'(t)^T \vec{T}(t) + \vec{B}(t)^T \vec{T}'(t) = 0
\end{equation} 

Mas $\vec{T}'(t) = K(t) \vec{N}(t)$, o que leva a:

\begin{equation}
	\vec{B}'(t)^T \vec{T}(t) + K(t)\vec{B}(t)^T \vec{N}(t) = 0
\end{equation} 

Como $\vec{B}(t)^T \vec{N}(t) = 0$, finalmente chega-se a:

\begin{equation}
	\vec{B}'(t)^T \vec{T}(t) = 0
\end{equation} mostrando que $c_1 = 0$. Portanto, como $\vec{B}'(t) \perp \vec{T}(t)$ e $\vec{B}'(t) \perp \vec{N}(t)$, devemos ter que $\vec{B}'(t)$ é paralelo ao vetor normal $\vec{N}(t)$:

\begin{equation}
	\vec{B}'(t) = c_2 \vec{N}(t)
\end{equation} onde, por convenção, $c_2 = - \tau(t)$ é o negativo da torção da curva, que mede o quão rápido a curva escapa do plano definido pelos vetores $\vec{T}(t)$ e $\vec{N}(t)$.

\subsubsection*{Variação do campo normal}

De modo similar, deseja-se expressar $\vec{N}'(t)$ em termos dos componentes do Triedro de Frenet:

\begin{equation}
	\vec{N}'(t) = c_1 \vec{T}(t) + c_2 \vec{N}(t) + c_3 \vec{B}(t)
\end{equation}

Primeiro, note que $\lVert \vec{N}(t) \rVert = 1$, de modo que derivando ambos os lados em relação a $t$, conclui-se que $\vec{N}'(t)^T \vec{N}(t) = 0$, o que implica em $c_2 = 0$. Sabe-se também que $\vec{N}(t)^T \vec{T}(t) = 0$ e diferenciação em relação a $t$ produz:

\begin{align}
	\vec{N}'(t)^T \vec{T}(t) + \vec{N}(t)^T \vec{T}'(t) & = 0 \\
	\vec{N}'(t)^T \vec{T}(t) + K(t)\vec{N}(t)^T \vec{N}(t) & = 0 \\
	\vec{N}'(t)^T \vec{T}(t) & = -K(t) 
\end{align}

A projeção da variação do vetor normal no vetor tangente é o negativo da curvatura, o que implica em $c_1 = -K(t)$. Lembrando que $\vec{N}(t)^T \vec{B}(t) = 0$, diferenciação em relação a $t$ produz:

\begin{align}
	\vec{N}'(t)^T \vec{B}(t) + \vec{N}(t)^T \vec{B}'(t) & = 0 \\
	\vec{N}'(t)^T \vec{B}(t) - \tau(t)\vec{N}(t)^T \vec{N}(t) & = 0 \\
	\vec{N}'(t)^T \vec{B}(t) & = \tau(t) 
\end{align} 

A projeção da variação do vetor normal no vetor binormal é a torsão, o que implica em $c_3 = \tau(t)$. Portanto, as equações de Frenet-Serret podem ser escritas como:

\begin{align}
	\vec{T}'(t) & = K(t) \vec{N}(t) \\
	\vec{N}'(t) & = -K(t) \vec{T}(t) + \tau(t)\vec{B}(t) \\
	\vec{B}'(t) & = - \tau(t)\vec{N}(t) 
\end{align} 

Em resumo, essas equações afirmam que:

\begin{itemize}
	\item A \textbf{variação} do \textbf{campo tangente} é expressa \textbf{somente} em termos da \textbf{curvatura} $K(t)$.
	\item A variação do campo normal é expressa em termos do negativo da curvatura, $-K(t)$, e da torção, $\tau(t)$.
	\item A variação do campo binormal é expressa somente em termos do negativo da torção, $-\tau(t)$.
\end{itemize}

A importância dessas equações é que no estudo e análise da forma de curvas paramétricas imersas em um espaço ambiente, se conhecermos a curvatura $K(t)$ e a torção $\tau(t)$ para todo ponto $t \in [a, b]$, então temos uma caracterização intrínseca completa de $\vec{\alpha}(t)$.

\subsection{Isometric Feature Mapping}
\noindent
ISOMAP foi um dos algoritmos pioneiros para aprendizado de variedades utilizados para redução de dimensionalidade. Os autores propuseram uma abordagem que combina as características da Análise de Componentes Principais (PCA) e \emph{Multidimensional Scaling} (MDS) \cite{MDS,MDS2} - eficiência computacional, otimalidade global, e garantias de convergência assintótica - com a flexibilidade de aprender uma ampla classe de variedades não lineares \cite{Isomap}. A ideia básica do algoritmo ISOMAP consiste em primeiramente construir um grafo KNN a partir das amostras para aproximar a variedade, computar os caminhos mínimos entre cada par de vértices do grafo para então, de posse das distâncias geodésicas aproximadas, encontrar um mapeamento para um espaço Euclidiano $R^d$ que preserva essas distâncias. É por essa razão que tais métodos são utilizados para realizar aprendizado não supervisionado de métricas. A Figura \ref{fig:PCA_vs_ISOMAP} ilustra a diferença entre as representações encontradas pelo PCA e pelo ISOMAP em um caso não linear. O algoritmo ISOMAP é capaz de aprender uma função de distância que é muito mais adequada para o conjunto de dados em questão.

\begin{figure}[ht]
\centerline{\includegraphics[scale=0.7]{./img/PCA_vs_ISOMAP.jpg}}
\vspace*{8pt}
\caption{Subespaço linear obtido pelo PCA versus o subespaço obtido pelo algoritmo ISOMAP. No ISOMAP, há preservação das distâncias, enquanto que no PCA, isso não ocorre. A projeção dos dados no subespaço PCA causa uma série de sobreposições entre amostras que no espaço de entrada estão distantes.}
\label{fig:PCA_vs_ISOMAP}
\end{figure}

A hipótese do algoritmo ISOMAP é que caminhos mínimos no grafo KNN são boas aproximações para as verdadeiras distâncias geodésicas na variedade. Pode-se mostrar que, sob certas condições de regularidade, o seguinte resultado é válido \cite{Isomap_converg}.

\begin{theorem}{(Teorema da Convergência Assintótica)}
	Dados $\lambda_1 , \lambda_2 , \mu > 0$, então, para um número suficientemente grande de amostras $\vec{x}_1, \vec{x}_2, ..., \vec{x}_n \in R^m$ a seguinte desigualdade:
	\begin{equation}
		(1 - \lambda_1) d_M(\vec{x}_i , \vec{x}_j) \leq d_G(\vec{x}_i , \vec{x}_j) \leq (1 + \lambda_2) d_M(\vec{x}_i , \vec{x}_j)
	\end{equation} é satisfeita com probabilidade $(1 - \mu)$, onde $d_G(\vec{x}_i , \vec{x}_j)$ é a distância da aproximação dos caminhos mínimos no grafo e $d_M(\vec{x}_i , \vec{x}_j)$ é a verdadeira distância geodésica na variedade.
\end{theorem}

Esse resultado mostra que de fato os comprimentos dos caminhos mínimos no grafo convergem para as distâncias geodésicas reais. O algoritmo ISOMAP pode ser dividido em 3 passos principais: 

\begin{enumerate}
	\item A partir dos dados de entrada $\vec{x}_1, \vec{x}_2, ..., \vec{x}_n \in R^m$ construir um grafo não direcionado de proximidade utilizando a regra KNN ou a regra $\epsilon$-ball \cite{Luxburg};
	\item Computar a matriz de distâncias ponto a ponto $D$ usando $n$ execuções do algoritmo de Dijkstra or ou uma execução do algoritmo Floyd-Warshall \cite{Cormen};
	\item Estimar as novas coordenadas dos pontos em um subespaço Euclidiano $R^d$ pela preservação das distâncias a partir do mpetodo \emph{Multidimensional Scaling} (MDS).
\end{enumerate}

\subsection{K-ISOMAP}
\noindent
Caminhos mínimos em grafos são aproximações discretas para curvas geodésicas em variedades. Ao se mover ao longo de uma curva, o Triedro de Frenet, composto pelos vetores tangente, normal e binormal, define uma base ortonormal que se adapta à curva em cada ponto, codificando informação relevante sobre sua forma.

Estendendo essa ideia, no método K-ISOMAP utiliza-se a base ortonormal que define o espaço tangente em um dado ponto da variedade como um referencial para extração de informação geométrica. Como a variação do campo tangente é expressa apenas em termos da curvatura $K(t)$, a proposta consiste em quantificar como o espaço tangente varia ao longo de uma curva geodésica, que no grafo é aproximada por um caminho mínimo, para construir uma função de distância intrínseca a ser utilizada na ponderação das arestas do grafo KNN. Chamamos esse grafo KNN definido em termos da curvatura como grafo-K ou \emph{K-graph}.

Seja $X = \{ \vec{x}_1 , \vec{x}_2 , \ldots, \vec{x}_n \}$, com $\vec{x}_i \in R^m$, a matriz de dados. O primeiro passo no método proposto consiste na construção do \emph{K-graph} a partir de $X$. Neste estágio inicial, adota-se a distância Euclidiana para computar os vizinhos mais próximos de cada amostra $\vec{x}_i$. Chamando de $\eta_i$ o sistema de vizinhança de $\vec{x}_i$, um patch $P_i$ é definido como o conjunto $\{ \vec{x}_i \cup \eta_i \}$. Note que o número de elementos de $P_i$ é $K+1$, para $i = 1, 2, ..., n$. Em notação matricial, um patch $P_i$ é dado por:

\begin{align}
	P_i  = \left[ \vec{x}_i , \vec{x}_{i1} , \vec{x}_{i2} , ... , \vec{x}_{ik} \right]  
		 = \begin{bmatrix}	
				x_i(1) & x_{i1}(1) & \ldots & x_{ik}(1)  \\ 
				x_i(2) & x_{i1}(2) & \ldots & x_{ik}(2) \\
				\vdots & \vdots & \ddots & \vdots  \\
				\vdots & \vdots & \ldots & \vdots  \\
				x_i(m) & x_{i1}(m) & \ldots & x_{ik}(m) 
		\end{bmatrix}_{m \times (k+1)}
\end{align} 

Para aproximar o espaço tangente em $\vec{x}_i$, o subespaço PCA é construído, o que define uma base ortonormal: 1) Primeiramente, computa-se a matriz de covariâncias das amostras de $P_i$, denotada por $\Sigma_i$; 2) Então, a matriz de covariâncias $\Sigma_i$ é decomposta em seus autovalores e autovetores:

\begin{equation}
	\Sigma_i = U_i Q_i U_i^T
\end{equation} onde $U_i = [\vec{u}_{i1}, \vec{u}_{i2}, \ldots, \vec{u}_{im}]$ é uma matriz $m \times m$ em que cada coluna representa um autovetor e $Q_i$ é uma matriz diagonal $m \times m$ com os autovalores de $\Sigma_i$. Note que as colunas de $U_i$ geram o espaço tangente em $\vec{x}_i$. O objetivo é quantificar a variação do espaço tangente ao percorrermos as arestas do grafo KNN. Neste projeto, são apresentadas duas abordagens para a estimação das curvaturas principais em um espaço tangente: A) Aproximação por diferenças finitas; e B) Aproximação baseada na geometria diferencial.

\subsubsection{Aproximação por diferenças finitas}

A definição de curvatura é a taxa de variação do vetor tangente ao longo da curva, ou seja:

\begin{equation}
	K(t) = \lVert \vec{T}'(t) \rVert
\end{equation}

Então, a maneira mais simples de direta de aproximá-la é usando um simples esquema de diferenças finitas. Seja $P = v_a v_1 v_2 \ldots v_b$ o caminho mínimo entre as amostras $\vec{x}_a$ e $\vec{x}_b$ no grafo KNN e seja  $P_i$ o patch referente a i-ésima amostras no caminho $P$. Ao calcular as matrizes de covariâncias $\Sigma_i$ e $\Sigma_j$ para a amostra $(v_i, v_j)$ do grafo KNN, pode-se encontrar os espaços tangentes em $\vec{x}_i$ e $\vec{x}_j$. Seja $U_i = \left[ \vec{u}_{i1} , \vec{u}_{i2} , \ldots, \vec{u}_{im} \right]$ e $U_j = \left[ \vec{u}_{j1} , \vec{u}_{j2} , \ldots, \vec{u}_{jm} \right]$ as bases ortogonais que definem os respectivos espaços tangentes. Então, uma aproximação ingênua para as curvaturas principais (uma vez que existem $m$ vetores tangentes) ao longo da aresta $(v_i, v_j)$ é dada por:

\begin{equation}
	K_{ij}^{(l)} = \lVert \vec{u}_{il} - \vec{u}_{jl} \rVert \qquad \text{for } l = 1, 2, ..., m
\end{equation}

Assim, $\vec{K}_{ij}$ é o vetor de curvaturas principais. A proposta consiste em substituir a distância Euclidiana (extrínseca) entre $\vec{x}_i$ e $\vec{x}_j$ pela norma do vetor de curvaturas principais, ou seja, $\lVert \vec{K}_{ij} \rVert$, como o peso da aresta $(v_i, v_j)$, gerando assim o \emph{K-graph}.

\subsubsection{Aproximação baseada na geometria diferencial}

Foi mostrado nas seções anteriores que a curvatura $K(t)$ pode ser computada exatamente por:

\begin{equation}
	K(t) = \frac{ \lVert \vec{\alpha}'(t) \times \vec{\alpha}''(t) \rVert }{ \lVert \vec{\alpha}'(t) \rVert^3 }
	\label{eq:K}
\end{equation}

Utilizando a mesma notação anterior, seja $U_i = \left[ \vec{u}_{i1} , \vec{u}_{i2} , \ldots, \vec{u}_{im} \right]$ e $U_j = \left[ \vec{u}_{j1} , \vec{u}_{j2} , \ldots, \vec{u}_{jm} \right]$ as bases ortogonais que definem os espaços tangentes em $\vec{x}_i$ e $\vec{x}_j$. Pode-se aproximar os vetores normais pelas diferenças entre os respectivos vetores tangentes:

\begin{equation}
	N = \left[ \vec{n}_1 , \vec{n}_2 , ... , \vec{n}_m \right] = \left[ \vec{u}_{i1} - \vec{u}_{j1}, \vec{u}_{i2} - \vec{u}_{j2}, \ldots, \vec{u}_{im} - \vec{u}_{jm} \right]
\end{equation}

Então, pode-se computar o numerador da equação \eqref{eq:K} para o $l$-ésimo vetor tangente como:

\begin{equation}
	\lVert \vec{u}_{il} \times \vec{n}_l \rVert = \lVert \vec{u}_{il} \rVert \lVert \vec{n}_{l} \rVert sin(\theta_l) \qquad \text{for } l = 1, 2, ..., m
\end{equation} onde o ângulo $\theta_l$ é obtido por:

\begin{equation}
	\theta_l = arccos \left( \frac{ \vec{u}_{il}^T \vec{n}_l }{ \lVert \vec{u}_{il} \rVert \lVert \vec{n}_{l} \rVert} \right) \qquad \text{for } l = 1, 2, ..., m
\end{equation}

O denominador da equação \eqref{eq:K} é dado por:

\begin{equation}
	\lVert \vec{u}_{il} \rVert^3 = \left( u_{il}(1)^2 + u_{il}(2)^2 + \cdots + u_{il}(m)^2 \right)^{3/2}
\end{equation}

Finalmente, os $m$ componentes do vetor de curvaturas principais são:

\begin{equation}
	K_{ij}^{(l)} = \frac{ \lVert \vec{u}_{il} \rVert \lVert \vec{n}_{l} \rVert sin(\theta_l)}{ \lVert \vec{u}_{il} \rVert^3 } \qquad \text{for } l = 1, 2, ..., m
\end{equation}

Novamente, a ideia consiste em substituir a distância Euclidiana (extrínseca) pela norma do vetor de curvaturas principais como o peso da aresta $(v_i, v_j)$ do grafo KNN, gerando uma segunda variação do \emph{K-graph}.

\subsection{Resultados preliminares}

Para avaliar o desempenho do método K-ISOMAP e compará-lo com outros algoritmos existentes para aprendizado não supervisionado de métricas via redução de dimensionalidade, um experimento com 36 conjuntos de dados obtidos no repositório online \url{www.openml.org}. O experimento foi organizado da seguinte maneira: primeiramente, 9 algoritmos para redução de dimensionalidade foram aplicados em cada um dos conjuntos de dados (PCA, Kernel PCA, ISOMAP, LLE, \emph{Laplacian Eigenmaps}, t-SNE, UMAP e as duas variantes do método K-ISOMAP), reduzindo a dimensão dos vetores de atributos para dois. Em seguida, para cada conjunto de dados, foram selecionadas arbitrariamente 50\% das amostras para compor o conjunto de treinamento e 50\% das amostras para compor o conjunto de testes. Depois, 8 diferentes classificadores supervisionados foram utilizados para medir a acurácia da classificação dos padrões (KNN com $K=7$, SVM, \emph{Naive Bayes}, \emph{Decision Trees}, \emph{Bayesian classifier under Gaussian hypothesis}, \emph{Multilayer Perceptron}, \emph{Random Forest Classifier} e \emph{Gaussian Process Classifier}), sendo que a acurácia média obtida dentre os oito classificadores é selecionada para cada conjunto de dados. Todos os resultados encontram-se na Tabela \ref{tab:acc}. É possível notar que em 32 dos 36 conjuntos de dados, uma das variações do K-ISOMAP obteve a melhor representação em termos de classificação, o que representa aproximadamente 88\% dos casos.

\begin{table*}[htb]
\scriptsize
  %%% Generated by mktable on Sat Dec 27 16:38:40 2008
  %%% Started by papa with args: mktable accuracies.txt
  \centering
  \caption{Acurácias médias obtidas por 8 classificadores supervisionados após redução de dimensionalidade com PCA, Kernel PCA, ISOMAP, LLE, Laplacian Eigenmaps, t-SNE, UMAP e K-ISOMAP (variantes A e B) para diversos conjuntos de dados do repositório \url{openML.org}  (caso 2-D).}
  \begin{tabular}{cccccccccc}
    \toprule
  & \textbf{PCA} & \textbf{KPCA} & \textbf{ISO} & \textbf{LLE}   & \textbf{LAP} & \textbf{tSNE} & \textbf{UMAP} & \textbf{K-ISO A} & \textbf{K-ISO B}   \\
\midrule
\textbf{iris}             & 0.930        & 0.825          & 0.893        & 0.795        & 0.667              & 0.926          & \textbf{0.968} & 0.950            & 0.965          \\
\textbf{prnn\_crabs}      & 0.595        & 0.582          & 0.597        & 0.583        & 0.558              & 0.738          & 0.789          & \textbf{0.822}   & \textbf{0.822} \\
\textbf{servo}            & 0.857        & 0.751          & 0.819        & 0.827        & 0.766              & 0.797          & 0.931          & 0.936            & \textbf{0.940} \\
\textbf{Tic-Tac-Toe}      & 0.644        & 0.713          & 0.646        & 0.636        & 0.726              & 0.765          & 0.799          & \textbf{0.818}   & 0.799          \\
\textbf{visualizing\_galaxy}           & 0.862        & \textbf{0.925} & 0.817        & 0.724        & 0.704              & 0.898          & 0.902          & 0.924            & 0.908          \\
\textbf{sleuth\_ex1605}           & 0.556        & 0.544          & 0.572        & 0.596        & 0.508              & 0.608          & 0.588          & \textbf{0.657}   & \textbf{0.657} \\
\textbf{mux6}             & 0.609        & 0.654          & 0.523        & 0.679        & 0.458              & 0.697          & 0.712          & \textbf{0.742}   & 0.699          \\
\textbf{Car}   & 0.756        & \textbf{0.800} & 0.652        & 0.681        & 0.687              & 0.697          & 0.676          & 0.797            & 0.789          \\
\textbf{blogger}          & 0.68         & 0.655          & 0.712        & 0.71         & 0.637              & 0.692          & 0.672          & 0.743            & \textbf{0.748} \\
\textbf{heart}         & 0.666        & 0.673          & 0.705        & 0.661        & 0.66               & 0.654          & 0.684          & 0.723            & \textbf{0.725} \\
\textbf{pyrim}            & 0.820        & 0.777          & 0.746        & 0.810        & 0.712              & 0.743          & 0.746          & \textbf{0.845}   & \textbf{0.845} \\
\textbf{SPECTF}           & 0.756        & 0.750          & 0.710        & 0.700        & 0.758              & 0.741          & 0.767          & \textbf{0.785}   & 0.775          \\
\textbf{veteran}          & 0.664        & 0.692          & 0.648        & 0.659        & 0.648              & 0.692          & 0.728          & \textbf{0.754}   & 0.726          \\
\textbf{balance-scale}    & 0.784        & 0.722          & 0.741        & 0.619        & 0.564              & 0.777          & 0.741          & 0.816            & \textbf{0.858} \\
\textbf{parity5}          & 0.445        & 0.460          & 0.359        & 0.367        & 0.398              & 0.476          & 0.320          & 0.547            & \textbf{0.664} \\
\textbf{kidney}           & 0.615        & 0.615          & 0.641        & 0.595        & 0.559              & 0.595          & 0.592          & \textbf{0.711}   & 0.691          \\
\textbf{Hayes-roth}       & 0.589        & 0.632          & 0.640        & 0.625        & 0.613              & 0.636          & 0.583          & 0.685            & \textbf{0.693} \\
\textbf{diabetes} & 0.562        & 0.545          & 0.556        & 0.647        & 0.534              & 0.613          & 0.585          & 0.733            & \textbf{0.750} \\
\textbf{parkinson}       & 0.855        & 0.836          & 0.772        & 0.778        & 0.797              & 0.852          & 0.840          & \textbf{0.857}   & 0.842          \\
\textbf{Rabe\_131}        & 0.740        & \textbf{0.950} & 0.840        & 0.760        & 0.750              & 0.735          & 0.914          & 0.950            & \textbf{0.960} \\
\textbf{corral}           & 0.831        & 0.845          & 0.826        & 0.703        & 0.751              & 0.848          & 0.754          & \textbf{0.902}   & 0.889          \\
\textbf{baskball}         & 0.638        & 0.643          & 0.570        & 0.598        & 0.544              & 0.609          & 0.606          & 0.708            & \textbf{0.711} \\
\textbf{Grub}      & 0.693        & 0.610          & 0.639        & 0.652        & 0.645              & 0.653          & 0.673          & \textbf{0.700}   & 0.685          \\
\textbf{bolts}            & 0.781        & 0.837          & 0.843        & 0.675        & 0.743              & 0.831          & 0.781          & 0.888            & \textbf{0.938} \\
\textbf{haberman}         & 0.752        & 0.718          & 0.745        & 0.732        & 0.723              & 0.734          & 0.742          & \textbf{0.765}   & 0.764          \\
\textbf{Tuning\_SVMs}       & 0.629        & 0.625          & 0.652        & 0.607        & 0.626              & 0.629          & 0.671          & 0.710            & \textbf{0.713} \\
\textbf{backache}         & 0.855        & 0.854          & 0.851        & 0.852        & 0.854              & 0.847          & 0.844          & 0.864            & \textbf{0.872} \\
\textbf{prnn\_synth}      & 0.858        & 0.832          & 0.863        & 0.770        & 0.703              & 0.843          & 0.848          & \textbf{0.867}   & 0.857          \\
\textbf{visualizing\_enviromental}     & 0.671        & 0.683          & 0.678        & 0.582        & 0.578              & 0.636          & 0.618          & \textbf{0.696}   & 0.676          \\
\textbf{mu284}            & 0.938        & 0.933          & 0.916        & 0.794        & 0.777              & 0.931          & 0.919          & 0.942            & \textbf{0.950} \\
\textbf{ar4}              & 0.803        & 0.810          & 0.789        & 0.782        & 0.775              & 0.782          & 0.814          & 0.820            & \textbf{0.824} \\
\textbf{Engine1}          & 0.793        & 0.848          & 0.861        & 0.705        & 0.775              & 0.879          & 0.846          & 0.882            & \textbf{0.895} \\
\textbf{prnn\_viruses}    & 0.783        & 0.764          & 0.806        & 0.741        & 0.677              & 0.691          & 0.797          & 0.852            & \textbf{0.857} \\
\textbf{vuneyard}         & 0.793        & 0.774          & 0.793        & 0.745        & 0.730              & 0.783          & 0.807          & \textbf{0.851}   & 0.836          \\
\textbf{confidence}       & 0.833        & 0.840          & 0.840        & 0.802        & 0.798              & 0.826          & 0.815          & \textbf{0.847}   & 0.845          \\
\textbf{user-knowledge}       & 0.506        & 0.498          & 0.570        & 0.442        & 0.455              & 0.631          & 0.629          & 0.656            & \textbf{0.676} \\
\midrule
\textbf{Average}          & 0.726        & 0.728          & 0.718        & 0.684        & 0.663              & 0.736          & 0.742          & 0.798   & \textbf{0.801} \\
\textbf{Median}           & 0.754        & 0.736          & 0.727        & 0.691        & 0.682              & 0.737          & 0.750          & \textbf{0.817}            & 0.811 \\
\textbf{Minimum}          & 0.445        & 0.460          & 0.359        & 0.367        & 0.398              & 0.476          & 0.320          & 0.547            & \textbf{0.657} \\
\textbf{Maximum}          & 0.938        & 0.950          & 0.916        & 0.852        & 0.854              & 0.931          & \textbf{0.968} & 0.950            & 0.965 \\
\bottomrule      
  \end{tabular}
  \label{tab:acc}
\end{table*}

Para testar se o desempenho do método K-ISOMAP é signficativamente melhor que o desempenho dos métodos tradicionais, testes de hipóteses não paramétricos serão conduzidos. Espera-se testar se há evidência de diferença entre os grupos com um teste de Kruskal-Wallis \cite{KruskalWallis} ou um teste de Friedman \cite{Friedman}. Posteriormente, um teste post-hoc de Nemenyi \cite{Nemenyi} será aplicado para descobrir quais grupos são signficativamente distintos e quais são equivalentes.

Uma comparação entre o desempenho do ISOMAP tradicional e do K-ISOMAP nesses conjuntos de dados revela que a incorporação de informação referente a curvatura local provocou um aumento médio de 13.6\% na acurácia da classificação. As melhorias mais perceptíveis foram observadas nos conjuntos de dados, parity5, prnn\_crabs e diabetes, com aumentos de 84.9\%, 37.6\% and 34.8\%, respectivamente. A Figura \ref{fig:graph} ilustra a comparação da acurácia média dos classificadores em cada dataset após aplicações do ISOMAP tradcional e do K-ISOMAP.

\begin{figure}[ht]
\begin{center}
\centerline{\includegraphics[scale=0.6]{./img/ISO_KISO.png}}
\caption{Comparação entre as acurácias médias de classificação obtidas em cada um dos 36 conjuntos de dados após redução de dimensionalidade com ISOMAP e K-ISOMAP.}
\label{fig:graph}
\end{center}
\end{figure}

Pretende-se investigar também a qualidade dos agrupamentos obtidos após as reduções de dimensionalidade a partir de indíces internos (não exige os rótulos das classes) como Silhouette Coefficient \cite{Silhouette}, e índices externos (exigem os rótulos das classes), como Rand Index \cite{Rand} e o escore de Fowlkes-Mallows \cite{Fowlkes}, que é definido como média geométrica da precisão (\emph{precision}) e da revocação (\emph{recall}).

Um dos aspectos positivos do método K-ISOMAP está relacionado com o \emph{out-of-sample problem} presente em vários algoritmos de aprendizado não supervisionado de métricas via redução de dimensionalidade. A maioria desses algoritmos não são capazes de lidar com novas amostras que não pertencem ao conjunto de treinamento de uma maneira simples e eficiente. A escolha natural é adicionar essas novas amostras ao conjunto e reaplicar o método novamente, o que computacionalmente pode se tornar bastante caro. Pode-se mostrar que o Kernel PCA se torna o ISOMAP quando a matriz de kernel $K(\vec{x}_i, \vec{x}_j)$ é definida como o negativo da metade da matriz de distâncias geodésicas \cite{KPCA}. Então, utilizando essa relação torna-se possível lidar com novas amostras no K-ISOMAP através da mesma estratégia de projeção do Kernel PCA. 

Uma limitação com o K-ISOMAP que requer uma análise mais aprofundada refere-se à definição do parâmetro $K$, que controla o número de vizinhos do grafo KNN (número de vizinhos). Os experimentos iniciais revelaram que a acurácia média dos classificadores pode ser sensível a mudanças no valor de $K$. Para a obtenção dos resultados preliminares mostrados na Tabela \ref{tab:acc}, foi empregada a seguinte estratégia: foram gerados os \emph{K-graphs} para todos os valores de $K$ pertencentes ao intervalo $[2, 25]$. O valor de $K$ que maximiza a acurácia média da classificação supervisionada é escolhido, sendo o processo repetido para cada conjunto de dados. Deve-se notar que os rótulos das classes não são utilizados pelo método K-ISOMAP, apenas para a seleção de modelos.

\section{Considerações Finais}
\label{sc:finais}

O aprendizado de métricas é um problema relevante para diversas aplicações que envolvem a utilização de alguma medida de similaridade, uma vez que conseguir aprender uma função de distância adequada entre objetos de uma coleção é fundamental na análise de dados multivariados. Neste projeto de pesquisa são apresentadas novas abordagens para o aprendizado de métricas não supervisionado utilizando redução de dimensionalidade a partir de medidas intrínsecas derivadas da geometria diferencial. Basicamente, a ideia consiste em deformar as arestas do grafo KNN de acordo com a geometria local da variedade: quanto maior a curvatura, mais a aresta é distorcida. Após passar por esse processo, o grafo KNN transforma-se no grafo-K (\emph{K-graph}). A suposição inicial de que métodos de aprendizado de métricas via redução de dimensionalidade podem se beneficiar da utilização dos \emph{K-graphs} baseiam-se em dois pontos fundamentais: 1) ao substituir a distância Euclidiana, que é uma medida de distância pontual, pela curvatura local, que é uma medida contextual baseada em patches, os algoritmos tendem a se tornar mais robustos em relação a presença de ruídos e \emph{outliers} nos dados; 2) resultados preliminares indicam que em certos casos, os atributos obtidos pelo método K-ISOMAP podem ser mais discriminantes que os atributos obtidos por outros algoritmos estado da arte em redução de dimensionalidade, como t-SNE e UMAP.

Recentemente, o aprendizado profundo tem sido considerado por muitos profissionais e pesquisadores como o estado da arte na criação de atributos a partir de conjuntos de dados de alta dimensionalidade, especialmente a partir de imagens. Um requisito para o aprendizado profundo funcionar de maneira adequada é ter um número muito grande de amostras para ajustar adequadamente os milhares de parâmetros existentes nas redes neurais profundas, o que nem sempre é viável do ponto de vista prático. Os algoritmos propostos nesse projeto, por outro lado, são capazes de aprender atributos discriminantes a partir de conjuntos de dados menores, produzindo bons resultados mesmo quando o número de amostras é menor ou igual ao número de atributos originais. Além disso, além de aprender uma representação mais compacta e significativa para o conjunto de dados observado, esses métodos também aprendem uma função de distância que geralmente é geometricamente mais adequada para representar uma medida de similaridade entre um par de objetos na coleção. Em outras palavras, aprendendo a estrutura oculta, em geral, ganhamos uma métrica mais poderosa. Portanto, não parece razoável supor que, eventualmente, o aprendizado profundo substitua todas as abordagens estatísticas de redução de dimensionalidade e aprendizado de métricas. Além disso, a maioria dos modelos de aprendizado profundo trabalha com o paradigma de aprendizado supervisionado, uma vez que são generalizações de perceptrons multicamadas, o que nem sempre é possível nas tarefas de reconhecimento de padrões. Por fim, os atributos produzidos por redes profundas carecem de semântica, no sentido de que é extremamente complicado explicar o significado de cada um deles. 

Trabalhos futuros podem incluir a utilização de outras variantes do PCA na estimação das bases ortogonais que definem os espaços tangentes, como: \emph{Robust PCA}, \emph{Sparse PCA} ou \emph{Parametric PCA}. Outras técnicas para a estimação da curvatura, como as adotadas em geometria computacional, podem ser empregadas de maneira alternativa. Uma versão supervisionada dos algoritmos de redução de dimensionalidade pode ser desenvolvida a partir da combinação das duas variantes de curvatura estimadas, de modo que para amostras vizinhas no grafo que pertencem à mesma classe, a menor curvatura principal é usada na ponderação da aresta, enquanto que para amostras vizinhas no grafo pertencentes à classes distintas, a soma das maiores curvaturas principais é usada para ponderar a aresta. Dessa forma, penaliza-se caminhos mínimos de cruzarem arestas que pertencem a fronteira de decisão, o que pode reduzir o erro na classificação supervisionada. Finalmente, é possível incorporar outros conceitos da geometria diferencial, como a torção dos caminhos mínimos, na derivação de novas medidas de distâncias intrínsecas.

\section{Cronograma}

O desenvolvimento deste projeto de pesquisa deve, a princípio, seguir as seguintes etapas, de acordo com um cronograma organizado em 12 bimestres, ou seja, 24 meses. A data de início do projeto está prevista para setembro de 2021 e a data de término está prevista para agosto de 2023.

\begin{enumerate}
	\item Concepção, formalização e derivação matemática do modelo de grafos baseados na curvatura local (K-graphs) a partir de conceitos da geometria diferencial e das equações de Frenet-Serret
	\item Desenvolvimento e implementação do modelo K-graphs.
	\item Desenvolvimento e implementação do método K-ISOMAP.
	\item Teste, validação e comparação do método K-ISOMAP com métodos similares na literatura.
	\item Desenvolvimento e implementação do método K-PCA.
	\item Teste, validação e comparação do método K-PCA com métodos similares na literatura.
	\item Desenvolvimento e implementação do método K-LLE
	\item Teste, validação e comparação do método K-LLE com métodos similares na literatura.
	\item Escrita e submissão de artigos para conferências nacionais e internacionais, bem como revistas qualificadas da área.
	\item Escrita do relatório final do projeto para a FAPESP.
\end{enumerate}


% tabela com 25 colunas e 15 linhas, caption de tabela vem acima da mesma.
\begin{table}[htbp]
\small
\caption{Cronograma de atividades previstas para a execução do projeto de pesquisa} % mude aqui para seu título da tabela
\begin{center}
\begin{tabular}{c|llllllllllll}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Atividades}}} & \multicolumn{12}{c}{\textbf{Trimestres}} \\ 
\multicolumn{1}{c|}{} & 01 & 02 & 03 & 04 & 05 & 06 & 07 & 08 & 09 & 10 & 11 & 12 \\ \midrule
1 & $\times$ & $\times$ & $\times$ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \midrule
2 & ~ & $\times$ & $\times$ & $\times$ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \midrule
3 & ~ & ~ & $\times$ & $\times$ & $\times$ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \midrule
4 & ~ & ~ & ~ & $\times$ & $\times$ & $\times$ & ~ & ~ & ~ & ~ & ~ & ~ \\ \midrule
5 & ~ & ~ & ~ & ~ & $\times$ & $\times$ & $\times$ & ~ & ~ & ~ & ~ & ~ \\ \midrule
6 & ~ & ~ & ~ & ~ & ~ & $\times$ & $\times$ & $\times$ & ~ & ~ & ~ & ~ \\ \midrule
7 & ~ & ~ & ~ & ~ & ~ & ~ & $\times$ & $\times$ & $\times$ & ~ & ~ & ~ \\ \midrule
8 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & $\times$ & $\times$ & $\times$ & ~ & ~ \\ \midrule
9 & ~ & ~ & ~ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ \\ \midrule
10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & $\times$ & $\times$ & $\times$ \\ \bottomrule
\end{tabular}
\end{center}
\label{cronograma} % para referencia no texto.
\end{table}

%% tabela com 25 colunas e 15 linhas, caption de tabela vem acima da mesma.
%\begin{table}[htbp]
%\caption{Cronograma de atividades previstas para a execução do projeto de pesquisa} % mude aqui para seu título da tabela
%\begin{center}
%\resizebox{\textwidth}{!}{ % abre resizebox, setar tabela da largura da página.
%\begin{tabular}{|c|l|l|l|l|l|l|l|l|l|l|l|l|}
%\hline
%\multicolumn{1}{|c|}{\multirow{2}{*}{Atividades}} & \multicolumn{12}{c|}{Trimestres} \\ \cline{2-13}
%\multicolumn{1}{|c|}{} & 01 & 02 & 03 & 04 & 05 & 06 & 07 & 08 & 09 & 10 & 11 & 12 \\ \hline
%1 & X & X & X & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
%2 & ~ & X & X & X & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
%3 & ~ & ~ & X & X & X & ~ & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
%4 & ~ & ~ & ~ & X & X & X & ~ & ~ & ~ & ~ & ~ & ~ \\ \hline
%5 & ~ & ~ & ~ & ~ & X & X & X & ~ & ~ & ~ & ~ & ~ \\ \hline
%6 & ~ & ~ & ~ & ~ & ~ & X & X & X & ~ & ~ & ~ & ~ \\ \hline
%7 & ~ & ~ & ~ & ~ & ~ & ~ & X & X & X & ~ & ~ & ~ \\ \hline
%8 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & X & X & X & ~ & ~ \\ \hline
%9 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & X & X & X & ~ \\ \hline
%10 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & X & X & X \\ \hline
%11 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & X & X & X & X & X \\ \hline
%12 & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & ~ & X & X & X \\ \hline
%13 & ~ & ~ & X & X & X & X & X & X & X & X & X & X \\ \hline
%\end{tabular}
%} % fecha resizebox
%\end{center}
%\label{cronograma} % para referencia no texto.
%\end{table}


\newpage
%\bibliographystyle{abntex2-alf}
\bibliographystyle{ieeetr}
\bibliography{mybib.bib}

\end{document}
