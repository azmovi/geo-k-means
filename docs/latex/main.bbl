\begin{thebibliography}{10}

\bibitem{Seung}
H.~S. Seung and D.~D. Lee, ``The manifold ways of perception,'' {\em Science},
  vol.~290, pp.~2268--2269, 2000.

\bibitem{Brand}
M.~Brand, {\em Charting a manifold}, vol.~15, pp.~961--968.
\newblock MIT Press, 2003.

\bibitem{Cayton}
L.~Cayton, ``{Algorithms for Manifold Learning},'' tech. rep., University of
  California San Diego (UCSD), 2005.

\bibitem{Huo2007}
X.~Huo, X.~Ni, and A.~K. Smith, {\em A Survey of Manifold-Based Learning
  Methods}, vol.~6 of {\em Series on Computers and Operations Research},
  pp.~691--745.
\newblock World Scientific, 2008.

\bibitem{Advances}
H.~Yin and W.~Huang, ``Adaptive nonlinear manifolds and their applications to
  pattern recognition,'' {\em Information Sciences}, vol.~180, no.~14,
  pp.~2649--2662, 2010.

\bibitem{Diffusion1}
R.~Talmon, I.~Cohen, S.~Gannot, and R.~R. Coifman, ``Diffusion maps for signal
  processing: A deeper look at manifold-learning techniques based on kernels
  and graphs,'' {\em IEEE Signal Processing Magazine}, vol.~30, no.~4,
  pp.~75--86, 2013.

\bibitem{Isomap}
J.~B. Tenenbaum, V.~de~Silva, and J.~C. Langford, ``A global geometric
  framework for nonlinear dimensionality reduction,'' {\em Science}, vol.~290,
  pp.~2319--2323, 2000.

\bibitem{Isomap_converg}
M.~Bernstein, V.~de~Silva, J.~Langford, and J.~Tenenbaum, ``Graph
  approximations to geodesics on embedded manifolds,'' 2000.

\bibitem{LLE}
S.~Roweis and L.~Saul, ``Nonlinear dimensionality reduction by locally linear
  embedding,'' {\em Science}, vol.~290, pp.~2323--2326, 2000.

\bibitem{LLE2}
L.~Saul and S.~Roweis, ``Think globally, fit locally: Unsupervised learning of
  low dimensional manifolds,'' {\em Journal of Machine Learning Research},
  vol.~4, pp.~119--155, 2003.

\bibitem{LLE3}
L.~K. Saul and S.~T. Roweis, ``An introduction to locally linear embedding,''
  tech. rep., New York University, 2000.

\bibitem{LLE4}
C.~Shalizi, ``Nonlinear dimensionality reduction i: Local linear embedding,''
  2009.
\newblock http://www.stat.cmu.edu/~cshalizi/350/lectures/14/lecture-14.pdf.

\bibitem{LLE5}
D.~de~Ridder and R.~P. Duin, ``Locally linear embedding for classification,''
  tech. rep., Delft University of Technology, 2002.

\bibitem{LapEig}
M.~Belkin and P.~Niyogi, ``Laplacian eigenmaps for dimensionality reduction and
  data representation,'' {\em Neural Computation}, vol.~15, pp.~1373--1396,
  June 2003.

\bibitem{SurveyLE}
L.~Bo, L.~Yan-Rui, and Z.~Xiao-Long, ``A survey on laplacian eigenmaps based
  manifold learning methods,'' {\em Neurocomputing}, vol.~335, pp.~336--351,
  2018.

\bibitem{Debie}
E.~Debie and K.~Shafi, ``Implications of the curse of dimensionality for
  supervised learning classifier systems: theoretical and empirical analyses,''
  {\em Pattern Analysis and Applications}, vol.~22, pp.~519--536, 2019.

\bibitem{Bellman:61}
R.~Bellman, {\em Adaptive Control Processes: A Guided Tour}.
\newblock Princeton University Press, 1961.

\bibitem{Fukunaga}
K.~Fukunaga, {\em Introduction to Statistical Pattern Recognition}.
\newblock Academic Press, 1990.

\bibitem{Scott92}
D.~W. Scott, {\em Multivariate Density Estimation}.
\newblock John Wiley \& Sons, 1992.

\bibitem{Hwang94}
J.~Hwang, S.~Lay, and A.~Lippman, ``Nonparametric multivariate density
  estimation: A comparative study,'' {\em IEEE Trans. on Signal Processing},
  vol.~42, no.~10, pp.~2795--2810, 1994.

\bibitem{Hughes68}
G.~F. Hughes, ``On the mean accuracy of statistical pattern recognizers,'' {\em
  IEEE Trans. on Information Theory}, vol.~14, pp.~55--63, 1968.

\bibitem{MetricLearning1}
D.~Li and Y.~Tian, ``Survey and experimental study on metric learning
  methods,'' {\em Neural Networks}, vol.~105, pp.~447--462, 2018.

\bibitem{MetricLearning2}
F.~Wang and J.~Sun, ``Survey on distance metric learning and dimensionality
  reduction in data mining,'' {\em Data Min. Knowl. Discov.}, vol.~29,
  pp.~534--564, Mar. 2015.

\bibitem{Verleysen}
J.~A. Lee and M.~Verleysen, {\em Nonlinear Dimensionality Reduction}.
\newblock Springer, 2007.

\bibitem{PPCA}
A.~L.~M. Levada, ``Parametric pca for unsupervised metric learning,'' {\em
  Pattern Recognition Letters}, vol.~135, pp.~425--430, 2020.

\bibitem{PCAKL}
A.~L.~M. Levada, ``Pca-kl: a parametric dimensionality reduction approach for
  unsupervised metric learning,'' {\em Advances in Data Analysis and
  Classification}, 2021.

\bibitem{PCA}
I.~T. Jolliffe, {\em Principal Component Analysis}.
\newblock Springer, 2~ed., 2002.

\bibitem{KPCA}
B.~Schölkopf, A.~Smola, and K.~R. Müller, ``Kernel principal component
  analysis,'' in {\em Advances in Kernel Methods – Support Vector Learning},
  pp.~327--352, MIT Press, 1999.

\bibitem{SparsePCA2006}
H.~Zou, T.~Hastie, and R.~Tibshirani, ``Sparse principal component analysis,''
  {\em Journal of Computational and Graphical Statistics}, vol.~15, no.~2,
  pp.~265--286, 2006.

\bibitem{RobustPCA}
E.~J. Cand\`{e}s, X.~Li, Y.~Ma, and J.~Wright, ``Robust principal component
  analysis?,'' {\em Journal of the ACM}, vol.~58, no.~3, 2011.

\bibitem{tSNE}
L.~van~der Maaten and G.~Hinton, ``Visualizing high-dimensional data using
  t-sne,'' {\em Journal of Machine Learning Research}, vol.~9, pp.~2579--2605,
  2008.

\bibitem{UMAP}
L.~McInnes, J.~Healy, N.~Saul, and L.~Großberger, ``Umap: Uniform manifold
  approximation and projection,'' {\em Journal of Open Source Software},
  vol.~3, no.~29, p.~861, 2018.

\bibitem{ISOKL}
A.~C. Neto and A.~L.~M. Levada, ``Isomap-kl: a parametric approach for
  unsupervised metric learning,'' in {\em 2020 33rd SIBGRAPI Conference on
  Graphics, Patterns and Images (SIBGRAPI)}, pp.~287--294, 2020.

\bibitem{ElementaryDG}
B.~O'Neill, {\em Elementary Differential Geometry}.
\newblock Elsevier, 2nd~ed., 2006.

\bibitem{FirstCourseDG}
T.~Shifrin, {\em Differential Geometry: A First Course in Curves and Surfaces}.
\newblock University of Georgia, 2016.

\bibitem{Manfredo}
M.~P. do~Carmo, {\em Differential Geometry of Curves and Surfaces}.
\newblock Dover Publications Inc., 2nd~ed., 2017.

\bibitem{Serret}
J.~A. Serret, ``Sur quelques formules relatives à la théorie des courbes à
  double courbure.,'' {\em J. de Math}, 1851.

\bibitem{Frenet}
F.~Frenet, ``Sur les courbes à double courbure.,'' {\em Abstract in J. de
  Math}, 1852.

\bibitem{MDS}
T.~F. Cox and M.~A.~A. Cox, {\em Multidimensional Scaling}, vol.~88 of {\em
  Monographs on Statistics and Applied Probability}.
\newblock Chapman \& Hall, 2001.

\bibitem{MDS2}
I.~Borg and P.~Groenen, {\em Modern Multidimensional Scaling: theory and
  applications}.
\newblock Springer-Verlag, 2~ed., 2005.

\bibitem{Luxburg}
U.~von Luxburg, ``A tutorial on spectral clustering,'' {\em Statistics and
  Computing}, vol.~17, pp.~395--416, 2007.

\bibitem{Cormen}
T.~H. Cormen, C.~E. Leiserson, R.~L. Rivest, and C.~Stein, {\em Introduction to
  Algorithms}.
\newblock MIT Press, 3~ed., 2009.

\bibitem{KruskalWallis}
W.~H. Kruskal and W.~A. Wallis, ``Use of ranks in one-criterion variance
  analysis,'' {\em Journal of the American Statistical Association}, vol.~47,
  no.~260, pp.~583--621, 1952.

\bibitem{Friedman}
M.~Friedman, ``The use of ranks to avoid the assumption of normality implicit
  in the analysis of variance,'' {\em Journal of the American Statistical
  Association}, vol.~32, no.~200, pp.~675--701, 1937.

\bibitem{Nemenyi}
P.~B. Nemenyi, {\em Distribution-free multiple comparisons}.
\newblock PhD thesis, Princeton University, 1963.

\bibitem{Silhouette}
P.~J. Rousseeuw, ``Silhouettes: A graphical aid to the interpretation and
  validation of cluster analysis,'' {\em Journal of Comp. and Appl. Math.},
  vol.~20, pp.~53--65, 1987.

\bibitem{Rand}
W.~M. Rand, ``Objective criteria for the evaluation of clustering methods,''
  {\em Journal of the American Statistical Association}, vol.~66, no.~336,
  pp.~846--850, 1971.

\bibitem{Fowlkes}
E.~B. Fowlkes and C.~L. Mallows, ``A method for comparing two hierarchical
  clusterings,'' {\em Journal of the American Statistical Association},
  vol.~78, no.~383, pp.~553--569, 1983.

\end{thebibliography}
